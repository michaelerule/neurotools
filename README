This is not a library, module or package. It should not be used by anyone, for any purpose, ever. 

Those of you familiar with computational and statistical science will know that much code is written and run only once. For most tasks, simply typing the relevant commands into an interactive Python, Matlab, or R terminal, will suffice. However, when we generate figures and run analyses, it is important to be able to go back and verify that said analysis was correct. This leads to the design pattern of saving one-off scripts, which are the list of commands run within a particular analysis session. Fancy things like the iPython notebook are instances of this design pattern. The fact that almost all code is written and run once creates unusual design constraints. Programmer time often trumps elegance, efficiency, and maintainability. Nevertheless, it is also critical that we be able to reference old code and verify its correctness, and run it again if need be. Therefore, some minimum standards of record keeping are absolutely necessary. I am still trying to solve this issue of version control in seldom run and frequently modified computational analysis code. Several solutions have occured to me, but I would also welcome critical input.

There are naturally some procedures which will be run many times. It is not always possible to know which ones in advance, but it is useful to have some migration path to convert one-off scripts into libraries, and then into optimized and debugged and well documented code. 
Code reuse in a mostly one-off workflow also presents some unusual constraints. Code may evolve in time, heuristic parameters may be updated or bugs may be fixed. It is therefore useful to know which version of a code-base generated any particular output, in order to keep accurate records and be able to determine if any bugs might have affected an analysis, or if parameters and algorithms have changed since the output was generated.

One solution is to save a snapshot of all the relevant code in a time-stamped directory along with the output. This leads to fragmentation of the codebase as old code is inevitably copied to run new analysis, and modified, leading to many redundant code branches. Version control is the obvious solution, and one can save the hash of a git commit with every analysis output in order to keep proper records. This does require that all users be familiar with git, and does depend on the git repository being properly maintained, and is more labor intensive. The primary purpose of this repository then is to provide a record such that the accuracy of any analysis can be publically verified, even if the original code that produced it has since been heavily revised.

Another feature to note is backwards compatibility when developing within these constraints. Very often, when one returns to a one-off script to re-use it, substantial improvements in the code quality are made. One re-reads through the code base, adding documentation and improving code readability, to verify that the once one-off script is suitable for a more general task. I have adopted the habit of converting any sequence of commands that is run more than once into a function somewhere in this library. However, as functions are edited, modified, and improved upon, compatibility with older scripts may be (necessarily) disrupted. If this were production code, we would have a suite of unit tests, and would be able to update older scripts to match changes in the code base. Working as a single individual, with code that is often run very few times, it is simply infeasible to maintain unit tests and enforce backwards compatibility. Typically, when some modification is made, it is tested with some simulated data to verify that it is functioning as expected. When possible, I have tried to preserve these test commands, although sometimes I test code interactively and have kept less than perfect records. 

An important caveat that is exacerbated in systems with permissive type systesm like Python and Matlab, is that changes in the codebase may cause old analysis code to run, but use an API improperly and return incorrect results. Changes in the expected orientation of a matrix, or the number, type, or order of paramters, are frequent culprits, as Matlab and Numpy will happily chew away at diverse numeric types without generating errors. To combat this I have adopted a particularly disruptive coding style. Whenever possible, any changes that modify how a function operates or is called are designed to cause old analysis code to crash completely. Typically, this is done by renaming the function so that the old code will fail entirely. Assertions, and changes in the order of parameters that will guarantee a type error down the line, can also be used to this effect. The general rule is that this entire code-base should be broken, all of the time, and that this is intentional. I simply do not have time to update the entire codebase with every change, and I rely on these inbuilt failures to allow me to maintain the codebase in a lazy fashion, only putting time into those parts of it which are actually needed in future analyses.

Additionally, this codebase contains code along the entire migration path from undocumented one-off scripts to well documented and thoroughly tested code. Unless there is community demand, I have no intention of changing this or documenting which portions of the codebase are stable. The entire purpose of this repository is to provide a version history and solve code reusability issues within my particular workflow. This repository is public so that if someone who knows what they are doing should come across it, and wish to fork portions of the code to save them time in developing their own analysis, they may do so. I make no guarantees on the accuracy of any of the code here. Code has been tested on simulated data for my particular analyses, and may fail in more general cases that I have not been able to test. I do ask, if you happen to come across a particularly grevious bug, in the form of a statistical routines that runs without error but returns an invalid result, that you let me know.




