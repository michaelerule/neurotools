

<!doctype html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>neurotools.regressions &mdash; neurotools 0 documentation</title>
    
    <link rel="stylesheet" href="../../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../_static/bizstyle.js"></script>
    <link rel="top" title="neurotools 0 documentation" href="../../index.html" />
    <link rel="up" title="Module code" href="../index.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">neurotools 0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for neurotools.regressions</h1><div class="highlight"><pre>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">I need to regress on the following model for synchrony</span>
<span class="sd">synchrony(x) = cos(wx)*exp(-x/tau)+b</span>

<span class="sd">which means that the angular synchrony cos(theta_x1-theta_x2) should</span>
<span class="sd">decay as a damped cosine, with some constant offset b. Note that</span>
<span class="sd">a nonzero constant offset may not indicate uniform synchrony, for </span>
<span class="sd">example, the direction of constant phase in a plane wave will contribute </span>
<span class="sd">a DC component.</span>

<span class="sd">Uses L2 penaly</span>

<span class="sd">X: List of distances</span>
<span class="sd">W: List of weights</span>
<span class="sd">Y: List of average pairwise distances</span>

<span class="sd">Model is cos(wx)*exp(-x/L)+b</span>
<span class="sd">Generates predictions Z</span>
<span class="sd">error is \sum W*(Z-Y)^2</span>

<span class="sd">gradient of the error </span>
<span class="sd">dErr/dw sum(W*(cos(w*x)*exp(-x/L)+b-Y)**2)</span>
<span class="sd">dErr/dL sum(W*(cos(w*x)*exp(-x/L)+b-Y)**2)</span>
<span class="sd">dErr/db sum(W*(cos(w*x)*exp(-x/L)+b-Y)**2)</span>

<span class="sd">sum(W* dErr/dw (cos(w*x)*exp(-x/L)+b-Y)**2)</span>
<span class="sd">sum(W* dErr/dL (cos(w*x)*exp(-x/L)+b-Y)**2)</span>
<span class="sd">sum(W* dErr/db (cos(w*x)*exp(-x/L)+b-Y)**2)</span>

<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y) dErr/dw (cos(w*x)*exp(-x/L)+b-Y))</span>
<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y) dErr/dL (cos(w*x)*exp(-x/L)+b-Y))</span>
<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y) dErr/db (cos(w*x)*exp(-x/L)+b-Y))</span>

<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y) * -sin(w*x)*exp(-x/L) )</span>
<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y) *  cos(w*x)*(-1/L)*exp(-x/L) )</span>
<span class="sd">sum(W* 2(cos(w*x)*exp(-x/L)+b-Y))</span>


<span class="sd">objective function is</span>

<span class="sd">def objective(w,L,b):</span>
<span class="sd">    z = cos(w*x)*exp(-x/L)+b</span>
<span class="sd">    error = sum( W*(z-Y)**2 )</span>
<span class="sd">    return error</span>

<span class="sd">def gradient(w,lambda,b):</span>
<span class="sd">    z = cos(w*x)*exp(-x/L)+b</span>
<span class="sd">    h = 2*(z-Y)</span>
<span class="sd">    dEdw = sum(W*h*-sin(w*x)*exp(-x/L))</span>
<span class="sd">    dEdL = sum(W*h* cos(w*x)*(-1/L)*exp(-x/L))</span>
<span class="sd">    dEdb = sum(W*H)</span>
<span class="sd">    return [dEdw,dEdL,dEdb]</span>

<span class="sd">We use the minimize function from scipy.optimize. </span>

<span class="sd">scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, </span>
<span class="sd">    hessp=None, bounds=None, constraints=(), tol=None, callback=None, </span>
<span class="sd">    options=None)</span>

<span class="sd">    Minimization of scalar function of one or more variables.</span>

<span class="sd">    New in version 0.11.0.</span>
<span class="sd">    Parameters:	</span>
<span class="sd">    fun : callable</span>
<span class="sd">        Objective function.</span>
<span class="sd">    x0 : ndarray</span>
<span class="sd">        Initial guess.</span>
<span class="sd">    args : tuple, optional</span>
<span class="sd">        Extra arguments passed to the objective function and its derivatives </span>
<span class="sd">        (Jacobian, Hessian).</span>
<span class="sd">    method : str or callable, optional</span>
<span class="sd">        Type of solver. Should be one of</span>
<span class="sd">            &#39;Nelder-Mead&#39;</span>
<span class="sd">            &#39;Powell&#39;</span>
<span class="sd">            &#39;CG&#39;</span>
<span class="sd">            &#39;BFGS&#39;</span>
<span class="sd">            &#39;Newton-CG&#39;</span>
<span class="sd">            &#39;Anneal (deprecated as of scipy version 0.14.0)&#39;</span>
<span class="sd">            &#39;L-BFGS-B&#39;</span>
<span class="sd">            &#39;TNC&#39;</span>
<span class="sd">            &#39;COBYLA&#39;</span>
<span class="sd">            &#39;SLSQP&#39;</span>
<span class="sd">            &#39;dogleg&#39;</span>
<span class="sd">            &#39;trust-ncg&#39;</span>
<span class="sd">            custom - a callable object (added in version 0.14.0)</span>
<span class="sd">        If not given, chosen to be one of BFGS, L-BFGS-B, SLSQP, d</span>
<span class="sd">        epending if the problem has constraints or bounds.</span>
<span class="sd">    jac : bool or callable, optional</span>
<span class="sd">        Jacobian (gradient) of objective function. Only for CG, BFGS, </span>
<span class="sd">        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If jac is a Boolean </span>
<span class="sd">        and is True, fun is assumed to return the gradient along with the </span>
<span class="sd">        objective function. If False, the gradient will be estimated </span>
<span class="sd">        numerically. jac can also be a callable returning the gradient of the </span>
<span class="sd">        objective. In this case, it must accept the same arguments as fun.</span>
<span class="sd">    hess, hessp : callable, optional</span>
<span class="sd">        Hessian (matrix of second-order derivatives) of objective function or </span>
<span class="sd">        Hessian of objective function times an arbitrary vector p. Only for </span>
<span class="sd">        Newton-CG, dogleg, trust-ncg. Only one of hessp or hess needs to be </span>
<span class="sd">        given. If hess is provided, then hessp will be ignored. If neither hess </span>
<span class="sd">        nor hessp is provided, then the Hessian product will be approximated </span>
<span class="sd">        using finite differences on jac. hessp must compute the Hessian times </span>
<span class="sd">        an arbitrary vector.</span>
<span class="sd">    bounds : sequence, optional</span>
<span class="sd">        Bounds for variables (only for L-BFGS-B, TNC and SLSQP). (min, max) </span>
<span class="sd">        pairs for each element in x, defining the bounds on that parameter. Use </span>
<span class="sd">        None for one of min or max when there is no bound in that direction.</span>
<span class="sd">    constraints : dict or sequence of dict, optional</span>
<span class="sd">        Constraints definition (only for COBYLA and SLSQP). Each constraint is </span>
<span class="sd">        defined in a dictionary with fields:</span>
<span class="sd">            type : str</span>
<span class="sd">                Constraint type: &#39;eq&#39; for equality, &#39;ineq&#39; for inequality.</span>
<span class="sd">            fun : callable</span>
<span class="sd">                The function defining the constraint.</span>
<span class="sd">            jac : callable, optional</span>
<span class="sd">                The Jacobian of fun (only for SLSQP).</span>
<span class="sd">            args : sequence, optional</span>
<span class="sd">                Extra arguments to be passed to the function and Jacobian.</span>
<span class="sd">        Equality constraint means that the constraint function result is to be </span>
<span class="sd">        zero whereas inequality means that it is to be non-negative. Note that </span>
<span class="sd">        COBYLA only supports inequality constraints.</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for termination. For detailed control, use solver-specific </span>
<span class="sd">        options.</span>
<span class="sd">    options : dict, optional</span>
<span class="sd">        A dictionary of solver options. All methods accept the following </span>
<span class="sd">        generic options:</span>
<span class="sd">            maxiter : int</span>
<span class="sd">                Maximum number of iterations to perform.</span>
<span class="sd">            disp : bool</span>
<span class="sd">                Set to True to print convergence messages.</span>
<span class="sd">        For method-specific options, see show_options.</span>
<span class="sd">    callback : callable, optional</span>
<span class="sd">        Called after each iteration, as callback(xk), where xk is the current </span>
<span class="sd">        parameter vector.</span>
<span class="sd">    Returns:	</span>
<span class="sd">    res : OptimizeResult</span>
<span class="sd">        The optimization result represented as a OptimizeResult object. </span>
<span class="sd">        Important attributes are: x the solution array, success a Boolean flag </span>
<span class="sd">        indicating if the optimizer exited successfully and message which </span>
<span class="sd">        describes the cause of the termination. See OptimizeResult for a </span>
<span class="sd">        description of other attributes.</span>

<span class="sd">&#39;&#39;&#39;</span>

<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<div class="viewcode-block" id="damped_cosine_regression"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.damped_cosine_regression">[docs]</a><span class="k">def</span> <span class="nf">damped_cosine_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">W</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X: List of distances</span>
<span class="sd">    W: List of weights</span>
<span class="sd">    Y: List of average pairwise distances</span>
<span class="sd">    </span>
<span class="sd">    todo: constrain b, L to be positive</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    # simple test</span>
<span class="sd">    X = 0.4*arange(9)</span>
<span class="sd">    Y = exp(-X/4+1)*cos(X)</span>
<span class="sd">    Z = Y+randn(*shape(X))</span>
<span class="sd">    W = ones(shape(X))</span>
<span class="sd">    w,L,b = damped_cosine_regression(X,Z,W).x</span>
<span class="sd">    plot(X,Y)</span>
<span class="sd">    plot(X,Z)</span>
<span class="sd">    plot(X,cos(w*X)*exp(-X/L+b))</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">((</span><span class="n">w</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">b</span><span class="p">)):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">/</span><span class="n">L</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="n">W</span><span class="o">*</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">((</span><span class="n">w</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">b</span><span class="p">)):</span>
        <span class="c"># todo: gradient is wrong?</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">/</span><span class="n">L</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>
        <span class="n">dEdw</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">h</span><span class="o">*-</span><span class="n">sin</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">/</span><span class="n">L</span><span class="p">))</span>
        <span class="n">dEdL</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">h</span><span class="o">*</span> <span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">L</span><span class="p">)</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">/</span><span class="n">L</span><span class="p">))</span>
        <span class="n">dEdb</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arr</span><span class="p">([</span><span class="n">dEdw</span><span class="p">,</span><span class="n">dEdL</span><span class="p">,</span><span class="n">dEdb</span><span class="p">])</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="c">#,jac=gradient)</span>
    <span class="k">return</span> <span class="n">result</span>
</div>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">linregress</span>
<div class="viewcode-block" id="weighted_least_squares"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.weighted_least_squares">[docs]</a><span class="k">def</span> <span class="nf">weighted_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">W</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Was using this one to initialize power law fit</span>
<span class="sd">    EPS = 1e-10</span>
<span class="sd">    use = (X&gt;EPS)&amp;(Y&gt;EPS)</span>
<span class="sd">    weighted_least_squares(log(X+EPS)[use],log(Y+EPS)[use],1/(EPS+X[use]))</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span> <span class="n">W</span><span class="o">*</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,[</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>

</div>
<div class="viewcode-block" id="power_law_regression"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.power_law_regression">[docs]</a><span class="k">def</span> <span class="nf">power_law_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">W</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Fit a power law, but with error terms computed by r^2 in </span>
<span class="sd">    the original space.</span>
<span class="sd">    </span>
<span class="sd">    power law form is</span>
<span class="sd">    </span>
<span class="sd">    log(y)  = a*log(x)+b</span>
<span class="sd">    or</span>
<span class="sd">    y = b*x^a</span>
<span class="sd">    </span>
<span class="sd">    initial best guess using linear regression</span>
<span class="sd">    </span>
<span class="sd">    minimize failing, just stick to weighted log-log linear regress</span>
<span class="sd">    </span>
<span class="sd">    result = power_law_regression(X,Y,1/X**16)</span>
<span class="sd">    a,b = result.x</span>

<span class="sd">    EPS = 1e-10</span>
<span class="sd">    use = (X&gt;EPS)&amp;(Y&gt;EPS)</span>
<span class="sd">    a,b = weighted_least_squares(log(X)[use],log(Y)[use],W[use]).x</span>
<span class="sd">    plot(sorted(X),b*arr(sorted(X))**a)</span>

<span class="sd">    from numpy.polynomial.polynomial import polyfit</span>

<span class="sd">    X,Y = ravel(f),ravel(y[:,i])</span>
<span class="sd">    a,b = power_law_regression(X,Y,1/X**2)</span>
<span class="sd">    plot(sorted(X),b*arr(sorted(X))**a)</span>
<span class="sd">    </span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">EPS</span> <span class="o">=</span> <span class="mf">1e-10</span>
    <span class="n">use</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&gt;</span><span class="n">EPS</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">Y</span><span class="o">&gt;</span><span class="n">EPS</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">polyfit</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="n">use</span><span class="p">],</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="n">use</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="n">w</span><span class="o">=</span><span class="n">W</span><span class="p">[</span><span class="n">use</span><span class="p">])</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    def objective((a,b)):</span>
<span class="sd">        z = exp(b+a*log(X))</span>
<span class="sd">        obj = sum((W*(Y-z)**2)[use])</span>
<span class="sd">        print a,b,obj</span>
<span class="sd">        return obj</span>
<span class="sd">    result = minimize(objective,[a,b])</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span><span class="n">exp</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>


</div>
<div class="viewcode-block" id="gaussian_function_regression"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.gaussian_function_regression">[docs]</a><span class="k">def</span> <span class="nf">gaussian_function_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X: List of distances</span>
<span class="sd">    Y: List of amplitudes</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">((</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span><span class="p">)):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">npdf</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span><span class="o">+</span><span class="n">dc</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>


</div>
<span class="kn">from</span> <span class="nn">neurotools.functions</span> <span class="kn">import</span> <span class="n">npdf</span>

<div class="viewcode-block" id="half_gaussian_function_regression"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.half_gaussian_function_regression">[docs]</a><span class="k">def</span> <span class="nf">half_gaussian_function_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X: List of distances</span>
<span class="sd">    Y: List of amplitudes</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">((</span><span class="n">sigma</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span><span class="p">)):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">npdf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span><span class="o">+</span><span class="n">dc</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">sigma</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">sigma</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span>

</div>
<div class="viewcode-block" id="exponential_decay_regression"><a class="viewcode-back" href="../../neurotools.regressions.html#neurotools.regressions.exponential_decay_regression">[docs]</a><span class="k">def</span> <span class="nf">exponential_decay_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    X: List of distances</span>
<span class="sd">    Y: List of amplitudes</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">((</span><span class="n">lamb</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span><span class="p">)):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lamb</span><span class="o">*</span><span class="n">X</span><span class="p">)</span><span class="o">*</span><span class="n">scale</span><span class="o">+</span><span class="n">dc</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">lamb</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">lamb</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">dc</span>
</pre></div></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">neurotools 0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, mrule.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>