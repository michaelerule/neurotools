

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>neurotools.models.rbm.rbm_utils.rbm &mdash; Neurotools 2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 

  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../../../index.html" class="icon icon-home"> Neurotools
          

          
          </a>

          
            
            
              <div class="version">
                2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../modules.html">neurotools</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Neurotools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
        
      <li>neurotools.models.rbm.rbm_utils.rbm</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for neurotools.models.rbm.rbm_utils.rbm</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/python</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">with_statement</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">nested_scopes</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">generators</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">unicode_literals</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="c1"># This code largely includes code written and copyrighted by the Theano</span>
<span class="c1"># developers and found at: http://deeplearning.net/tutorial/code/rbm.py</span>
<span class="c1"># The licence notice for that code is found at:</span>
<span class="c1"># http://deeplearning.net/tutorial/LICENSE.html?highlight=license</span>

<span class="kn">import</span> <span class="nn">timeit</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">theano</span>
    <span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">T</span>
    <span class="c1">#from theano.tensor.shared_randomstreams import RandomStreams</span>
    <span class="kn">from</span> <span class="nn">theano.sandbox.rng_mrg</span> <span class="k">import</span> <span class="n">MRG_RandomStreams</span> <span class="k">as</span> <span class="n">RandomStreams</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;theano is missing; rbmutils will not work&#39;</span><span class="p">)</span>
    

<div class="viewcode-block" id="LoadRBM"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.LoadRBM">[docs]</a><span class="k">def</span> <span class="nf">LoadRBM</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="c1"># raise NotImplementedError(&quot;Need to check if we need W or W.T!&quot;)</span>
    <span class="k">with</span> <span class="n">numpy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">file_data</span><span class="p">:</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">file_data</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
        <span class="n">hbias</span> <span class="o">=</span> <span class="n">file_data</span><span class="p">[</span><span class="s1">&#39;hidbias&#39;</span><span class="p">]</span>
        <span class="n">vbias</span> <span class="o">=</span> <span class="n">file_data</span><span class="p">[</span><span class="s1">&#39;visbias&#39;</span><span class="p">]</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="n">vbias</span><span class="o">.</span><span class="n">size</span>
    <span class="n">hs</span> <span class="o">=</span> <span class="n">hbias</span><span class="o">.</span><span class="n">size</span>
    <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="n">hs</span><span class="p">),</span> <span class="s2">&quot;Inconsistent weight matrix&quot;</span>
    <span class="n">rbm</span> <span class="o">=</span> <span class="n">RBM</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">hs</span><span class="p">,</span> <span class="n">W</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">hbias</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">vbias</span><span class="o">*</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rbm</span></div>


<div class="viewcode-block" id="RBM"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM">[docs]</a><span class="k">class</span> <span class="nc">RBM</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Restricted Boltzmann Machine (RBM)  &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">n_visible</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
        <span class="n">n_hidden</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hbias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">vbias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">numpy_rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">theano_rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reg_weight</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
        <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        RBM constructor. Defines the parameters of the model along with</span>
<span class="sd">        basic operations for inferring hidden from visible (and vice-versa),</span>
<span class="sd">        as well as for performing CD updates.</span>

<span class="sd">        :param input: None for standalone RBMs or symbolic variable if RBM is</span>
<span class="sd">        part of a larger graph.</span>

<span class="sd">        :param n_visible: number of visible units</span>

<span class="sd">        :param n_hidden: number of hidden units</span>

<span class="sd">        :param W: None for standalone RBMs or symbolic variable pointing to a</span>
<span class="sd">        shared weight matrix in case RBM is part of a DBN network; in a DBN,</span>
<span class="sd">        the weights are shared between RBMs and layers of a MLP</span>

<span class="sd">        :param hbias: None for standalone RBMs or symbolic variable pointing</span>
<span class="sd">        to a shared hidden units bias vector in case RBM is part of a</span>
<span class="sd">        different network</span>

<span class="sd">        :param vbias: None for standalone RBMs or a symbolic variable</span>
<span class="sd">        pointing to a shared visible units bias</span>

<span class="sd">        :param reg_weight: weight of a regularisatio term</span>
<span class="sd">        (add by hand to cost function)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span> <span class="o">=</span> <span class="n">n_visible</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>

        <span class="k">if</span> <span class="n">numpy_rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># create a number generator</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">numpy_rng</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">numpy_rng</span> <span class="o">=</span> <span class="n">numpy_rng</span>

        <span class="k">if</span> <span class="n">theano_rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">theano_rng</span> <span class="o">=</span> <span class="n">RandomStreams</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy_rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">30</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># W is initialized with `initial_W` which is uniformely</span>
            <span class="c1"># sampled from -4*sqrt(6./(n_visible+n_hidden)) and</span>
            <span class="c1"># 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if</span>
            <span class="c1"># converted using asarray to dtype theano.config.floatX so</span>
            <span class="c1"># that the code is runable on GPU</span>
            <span class="n">initial_W</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">numpy_rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
                    <span class="n">low</span><span class="o">=-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_hidden</span> <span class="o">+</span> <span class="n">n_visible</span><span class="p">)),</span>
                    <span class="n">high</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_hidden</span> <span class="o">+</span> <span class="n">n_visible</span><span class="p">)),</span>
                    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_visible</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">)</span>
            <span class="c1"># theano shared variables for weights and biases</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">initial_W</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hbias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># create shared variable for hidden units bias</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">n_hidden</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
                <span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;hbias&#39;</span><span class="p">,</span>
                <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">hbias</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;hbias&#39;</span><span class="p">,</span>
                <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">vbias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># create shared variable for visible units bias</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">n_visible</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
                <span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vbias&#39;</span><span class="p">,</span>
                <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">vbias</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vbias&#39;</span><span class="p">,</span>
                <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

        <span class="c1"># initialize input layer for standalone RBM or layer0 of DBN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">input</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">theano_rng</span> <span class="o">=</span> <span class="n">theano_rng</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg_weight</span> <span class="o">=</span> <span class="n">reg_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

<div class="viewcode-block" id="RBM.free_energy"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.free_energy">[docs]</a>    <span class="k">def</span> <span class="nf">free_energy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; Function to compute the free energy &#39;&#39;&#39;</span>
        <span class="n">wx_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span>
        <span class="n">vbias_term</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v_sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span><span class="p">)</span>
        <span class="n">hidden_term</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">wx_b</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">hidden_term</span> <span class="o">-</span> <span class="n">vbias_term</span></div>

<div class="viewcode-block" id="RBM.propup"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.propup">[docs]</a>    <span class="k">def</span> <span class="nf">propup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vis</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;This function propagates the visible units activation upwards to</span>
<span class="sd">        the hidden units</span>

<span class="sd">        Note that we return also the pre-sigmoid activation of the</span>
<span class="sd">        layer. As it will turn out later, due to how Theano deals with</span>
<span class="sd">        optimizations, this symbolic variable will be needed to write</span>
<span class="sd">        down a more stable computational graph (see details in the</span>
<span class="sd">        reconstruction cost function)</span>

<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">pre_sigmoid_activation</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_activation</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pre_sigmoid_activation</span><span class="p">)]</span></div>

<div class="viewcode-block" id="RBM.sample_h_given_v"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.sample_h_given_v">[docs]</a>    <span class="k">def</span> <span class="nf">sample_h_given_v</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v0_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; This function infers state of hidden units given visible units &#39;&#39;&#39;</span>
        <span class="c1"># compute the activation of the hidden units given a sample of</span>
        <span class="c1"># the visibles</span>
        <span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propup</span><span class="p">(</span><span class="n">v0_sample</span><span class="p">)</span>
        <span class="c1"># get a sample of the hiddens given their activation</span>
        <span class="c1"># Note that theano_rng.binomial returns a symbolic sample of dtype</span>
        <span class="c1"># int64 by default. If we want to keep our computations in floatX</span>
        <span class="c1"># for the GPU we need to specify to return the dtype floatX</span>
        <span class="n">h1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theano_rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">h1_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                             <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h1_mean</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span><span class="p">,</span> <span class="n">h1_sample</span><span class="p">]</span></div>

<div class="viewcode-block" id="RBM.propdown"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.propdown">[docs]</a>    <span class="k">def</span> <span class="nf">propdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hid</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;This function propagates the hidden units activation downwards to</span>
<span class="sd">        the visible units</span>

<span class="sd">        Note that we return also the pre_sigmoid_activation of the</span>
<span class="sd">        layer. As it will turn out later, due to how Theano deals with</span>
<span class="sd">        optimizations, this symbolic variable will be needed to write</span>
<span class="sd">        down a more stable computational graph (see details in the</span>
<span class="sd">        reconstruction cost function)</span>

<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">pre_sigmoid_activation</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_activation</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pre_sigmoid_activation</span><span class="p">)]</span></div>

<div class="viewcode-block" id="RBM.sample_v_given_h"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.sample_v_given_h">[docs]</a>    <span class="k">def</span> <span class="nf">sample_v_given_h</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h0_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; This function infers state of visible units given hidden units &#39;&#39;&#39;</span>
        <span class="c1"># compute the activation of the visible given the hidden sample</span>
        <span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propdown</span><span class="p">(</span><span class="n">h0_sample</span><span class="p">)</span>
        <span class="c1"># get a sample of the visible given their activation</span>
        <span class="c1"># Note that theano_rng.binomial returns a symbolic sample of dtype</span>
        <span class="c1"># int64 by default. If we want to keep our computations in floatX</span>
        <span class="c1"># for the GPU we need to specify to return the dtype floatX</span>
        <span class="n">v1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theano_rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">v1_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                             <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">v1_mean</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span><span class="p">,</span> <span class="n">v1_sample</span><span class="p">]</span></div>

<div class="viewcode-block" id="RBM.gibbs_hvh"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.gibbs_hvh">[docs]</a>    <span class="k">def</span> <span class="nf">gibbs_hvh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h0_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; This function implements one step of Gibbs sampling,</span>
<span class="sd">            starting from the hidden state&#39;&#39;&#39;</span>
        <span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span><span class="p">,</span> <span class="n">v1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_v_given_h</span><span class="p">(</span><span class="n">h0_sample</span><span class="p">)</span>
        <span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span><span class="p">,</span> <span class="n">h1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_h_given_v</span><span class="p">(</span><span class="n">v1_sample</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span><span class="p">,</span> <span class="n">v1_sample</span><span class="p">,</span>
                <span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span><span class="p">,</span> <span class="n">h1_sample</span><span class="p">]</span></div>

<div class="viewcode-block" id="RBM.gibbs_vhv"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.gibbs_vhv">[docs]</a>    <span class="k">def</span> <span class="nf">gibbs_vhv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v0_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; This function implements one step of Gibbs sampling,</span>
<span class="sd">            starting from the visible state&#39;&#39;&#39;</span>
        <span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span><span class="p">,</span> <span class="n">h1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_h_given_v</span><span class="p">(</span><span class="n">v0_sample</span><span class="p">)</span>
        <span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span><span class="p">,</span> <span class="n">v1_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_v_given_h</span><span class="p">(</span><span class="n">h1_sample</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span><span class="p">,</span> <span class="n">h1_sample</span><span class="p">,</span>
                <span class="n">pre_sigmoid_v1</span><span class="p">,</span> <span class="n">v1_mean</span><span class="p">,</span> <span class="n">v1_sample</span><span class="p">]</span></div>

<div class="viewcode-block" id="RBM.mean_h_given_v"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.mean_h_given_v">[docs]</a>    <span class="k">def</span> <span class="nf">mean_h_given_v</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v0_sample</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39; This function infers state of hidden units given visible units &#39;&#39;&#39;</span>
        <span class="c1"># compute the activation of the hidden units given a sample of</span>
        <span class="c1"># the visibles</span>
        <span class="n">pre_sigmoid_h1</span><span class="p">,</span> <span class="n">h1_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propup</span><span class="p">(</span><span class="n">v0_sample</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1_mean</span><span class="p">)</span></div>

<div class="viewcode-block" id="RBM.get_cost_updates"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.get_cost_updates">[docs]</a>    <span class="k">def</span> <span class="nf">get_cost_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This functions implements one step of CD-k or PCD-k</span>

<span class="sd">        :param lr: learning rate used to train the RBM</span>

<span class="sd">        :param persistent: None for CD. For PCD, shared variable</span>
<span class="sd">            containing old state of Gibbs chain. This must be a shared</span>
<span class="sd">            variable of size (batch size, number of hidden units).</span>

<span class="sd">        :param k: number of Gibbs steps to do in CD-k/PCD-k</span>

<span class="sd">        Returns a proxy for the cost and the updates dictionary. The</span>
<span class="sd">        dictionary contains the update rules for weights and biases but</span>
<span class="sd">        also an update of the shared variable used to store the persistent</span>
<span class="sd">        chain, if one is used.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># compute positive phase</span>
        <span class="n">pre_sigmoid_ph</span><span class="p">,</span> <span class="n">ph_mean</span><span class="p">,</span> <span class="n">ph_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_h_given_v</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>

        <span class="c1"># decide how to initialize persistent chain:</span>
        <span class="c1"># for CD, we use the newly generate hidden sample</span>
        <span class="c1"># for PCD, we initialize from the old state of the chain</span>
        <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">chain_start</span> <span class="o">=</span> <span class="n">ph_sample</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chain_start</span> <span class="o">=</span> <span class="n">persistent</span>
        <span class="c1"># perform actual negative phase</span>
        <span class="c1"># in order to implement CD-k/PCD-k we need to scan over the</span>
        <span class="c1"># function that implements one gibbs step k times.</span>
        <span class="c1"># Read Theano tutorial on scan for more information :</span>
        <span class="c1"># http://deeplearning.net/software/theano/library/scan.html</span>
        <span class="c1"># the scan will return the entire Gibbs chain</span>
        <span class="p">(</span>
            <span class="p">[</span>
                <span class="n">pre_sigmoid_nvs</span><span class="p">,</span>
                <span class="n">nv_means</span><span class="p">,</span>
                <span class="n">nv_samples</span><span class="p">,</span>
                <span class="n">pre_sigmoid_nhs</span><span class="p">,</span>
                <span class="n">nh_means</span><span class="p">,</span>
                <span class="n">nh_samples</span>
            <span class="p">],</span>
            <span class="n">updates</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gibbs_hvh</span><span class="p">,</span>
            <span class="c1"># the None are place holders, saying that</span>
            <span class="c1"># chain_start is the initial state corresponding to the</span>
            <span class="c1"># 6th output</span>
            <span class="n">outputs_info</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">chain_start</span><span class="p">],</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="n">k</span>
        <span class="p">)</span>
        <span class="c1"># determine gradients on RBM parameters</span>
        <span class="c1"># note that we only need the sample at the end of the chain</span>
        <span class="n">chain_end</span> <span class="o">=</span> <span class="n">nv_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_weight</span><span class="o">==</span><span class="mf">0.0</span><span class="p">:</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">))</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="n">chain_end</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;no regulariser&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">))</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="n">chain_end</span><span class="p">))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_weight</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;with L2 on weights&#39;</span><span class="p">)</span>
            <span class="c1">#cost = T.mean(self.free_energy(self.input)) - T.mean(self.free_energy(chain_end)) + self.reg_weight * self.mean_h_given_v(self.input)</span>
            <span class="c1">#print(&#39;with sparseness regulariser hidden biases&#39;)</span>


        <span class="c1"># We must not compute the gradient through the gibbs sampling</span>
        <span class="n">gparams</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">consider_constant</span><span class="o">=</span><span class="p">[</span><span class="n">chain_end</span><span class="p">])</span>

        <span class="c1"># constructs the update dictionary</span>
        <span class="k">for</span> <span class="n">gparam</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gparams</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="c1"># make sure that the learning rate is of the right dtype</span>
            <span class="n">updates</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span> <span class="o">-</span> <span class="n">gparam</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
                <span class="n">lr</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="c1"># Note that this works only if persistent is a shared variable</span>
            <span class="n">updates</span><span class="p">[</span><span class="n">persistent</span><span class="p">]</span> <span class="o">=</span> <span class="n">nh_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># pseudo-likelihood is a better proxy for PCD</span>
            <span class="n">monitoring_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_pseudo_likelihood_cost</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># reconstruction cross-entropy is a better proxy for CD</span>
            <span class="n">monitoring_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reconstruction_cost</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span>
                                                           <span class="n">pre_sigmoid_nvs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">monitoring_cost</span><span class="p">,</span> <span class="n">updates</span></div>

<div class="viewcode-block" id="RBM.get_pseudo_likelihood_cost"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.get_pseudo_likelihood_cost">[docs]</a>    <span class="k">def</span> <span class="nf">get_pseudo_likelihood_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Stochastic approximation to the pseudo-likelihood&quot;&quot;&quot;</span>

        <span class="c1"># index of bit i in expression p(x_i | x_{\i})</span>
        <span class="n">bit_i_idx</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bit_i_idx&#39;</span><span class="p">)</span>

        <span class="c1"># binarize the input image by rounding to nearest integer</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>

        <span class="c1"># calculate free energy for the given bit configuration</span>
        <span class="n">fe_xi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>

        <span class="c1"># flip bit x_i of matrix xi and preserve all other bits x_{\i}</span>
        <span class="c1"># Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns</span>
        <span class="c1"># the result to xi_flip, instead of working in place on xi.</span>
        <span class="n">xi_flip</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">set_subtensor</span><span class="p">(</span><span class="n">xi</span><span class="p">[:,</span> <span class="n">bit_i_idx</span><span class="p">],</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">xi</span><span class="p">[:,</span> <span class="n">bit_i_idx</span><span class="p">])</span>

        <span class="c1"># calculate free energy with bit flipped</span>
        <span class="n">fe_xi_flip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">free_energy</span><span class="p">(</span><span class="n">xi_flip</span><span class="p">)</span>

        <span class="c1"># equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\i})))</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">fe_xi_flip</span> <span class="o">-</span>
                                                            <span class="n">fe_xi</span><span class="p">)))</span>

        <span class="c1"># increment bit_i_idx % number as part of updates</span>
        <span class="n">updates</span><span class="p">[</span><span class="n">bit_i_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">bit_i_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span>

        <span class="k">return</span> <span class="n">cost</span></div>

<div class="viewcode-block" id="RBM.get_reconstruction_cost"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.get_reconstruction_cost">[docs]</a>    <span class="k">def</span> <span class="nf">get_reconstruction_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">pre_sigmoid_nv</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Approximation to the reconstruction error</span>

<span class="sd">        Note that this function requires the pre-sigmoid activation as</span>
<span class="sd">        input.  To understand why this is so you need to understand a</span>
<span class="sd">        bit about how Theano works. Whenever you compile a Theano</span>
<span class="sd">        function, the computational graph that you pass as input gets</span>
<span class="sd">        optimized for speed and stability.  This is done by changing</span>
<span class="sd">        several parts of the subgraphs with others.  One such</span>
<span class="sd">        optimization expresses terms of the form log(sigmoid(x)) in</span>
<span class="sd">        terms of softplus.  We need this optimization for the</span>
<span class="sd">        cross-entropy since sigmoid of numbers larger than 30. (or</span>
<span class="sd">        even less then that) turn to 1. and numbers smaller than</span>
<span class="sd">        -30. turn to 0 which in terms will force theano to compute</span>
<span class="sd">        log(0) and therefore we will get either -inf or NaN as</span>
<span class="sd">        cost. If the value is expressed in terms of softplus we do not</span>
<span class="sd">        get this undesirable behaviour. This optimization usually</span>
<span class="sd">        works fine, but here we have a special case. The sigmoid is</span>
<span class="sd">        applied inside the scan op, while the log is</span>
<span class="sd">        outside. Therefore Theano will only see log(scan(..)) instead</span>
<span class="sd">        of log(sigmoid(..)) and will not apply the wanted</span>
<span class="sd">        optimization. We can not go and replace the sigmoid in scan</span>
<span class="sd">        with something else also, because this only needs to be done</span>
<span class="sd">        on the last step. Therefore the easiest and more efficient way</span>
<span class="sd">        is to get also the pre-sigmoid activation as an output of</span>
<span class="sd">        scan, and apply both the log and sigmoid outside scan such</span>
<span class="sd">        that Theano can catch and optimize the expression.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pre_sigmoid_nv</span><span class="p">))</span> <span class="o">+</span>
                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pre_sigmoid_nv</span><span class="p">)),</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">cross_entropy</span></div>

<div class="viewcode-block" id="RBM.train"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_x</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
              <span class="n">training_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
              <span class="n">output_all</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">output_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">cd_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param data_x: the training set</span>

<span class="sd">        :param learning_rate: learning rate used for training the RBM</span>

<span class="sd">        :param training_epochs: number of epochs used for training</span>

<span class="sd">        :param batch_size: size of a batch used to train the RBM</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>

        <span class="n">data_len</span><span class="p">,</span> <span class="n">n_vis</span> <span class="o">=</span> <span class="n">data_x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">train_set_x</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span>
                                                  <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>

        <span class="c1"># compute number of minibatches for training, validation and testing</span>
        <span class="n">n_train_batches</span> <span class="o">=</span> <span class="n">train_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> \
            <span class="o">//</span> <span class="n">batch_size</span>

        <span class="c1"># allocate symbolic variables for the data</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span>    <span class="c1"># index to a [mini]batch</span>

        <span class="c1"># initialize storage for the persistent chain (state = hidden</span>
        <span class="c1"># layer of chain)</span>
        <span class="n">persistent_chain</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">),</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
            <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># get the cost and the gradient corresponding to one step of CD-k</span>
        <span class="n">cost</span><span class="p">,</span> <span class="n">updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_cost_updates</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                              <span class="n">persistent</span><span class="o">=</span><span class="n">persistent_chain</span><span class="p">,</span>
                                              <span class="n">k</span><span class="o">=</span><span class="n">cd_steps</span><span class="p">)</span>

        <span class="n">pseudo_likelihoods</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">training_epochs</span><span class="p">)</span>

        <span class="c1">#################################</span>
        <span class="c1">#     Training the RBM          #</span>
        <span class="c1">#################################</span>

        <span class="c1"># it is ok for a theano function to have no output</span>
        <span class="c1"># the purpose of train_rbm is solely to update the RBM parameters</span>
        <span class="n">train_rbm</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
            <span class="p">[</span><span class="n">index</span><span class="p">],</span>
            <span class="n">cost</span><span class="p">,</span>
            <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
            <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
                <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
            <span class="p">},</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_rbm&#39;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;... training started, &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; batches&#39;</span><span class="p">)</span>
        <span class="n">plotting_time</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>

        <span class="c1"># go through training epochs</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_epochs</span><span class="p">):</span>

            <span class="c1"># save parameters before each training epoch</span>
            <span class="k">if</span> <span class="n">output_all</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">plotting_start</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
                <span class="n">visbias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">hidbias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">numpy</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">output_all</span> <span class="o">+</span> <span class="s1">&#39;/pars_at_epoch</span><span class="si">%i</span><span class="s1">.npy&#39;</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">,</span>
                           <span class="p">[</span><span class="n">visbias</span><span class="p">,</span> <span class="n">hidbias</span><span class="p">,</span> <span class="n">weights</span><span class="p">])</span>
                <span class="n">plotting_stop</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
                <span class="n">plotting_time</span> <span class="o">+=</span> <span class="p">(</span><span class="n">plotting_stop</span> <span class="o">-</span> <span class="n">plotting_start</span><span class="p">)</span>

            <span class="c1"># go through the training set</span>
            <span class="n">mean_cost</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">):</span>
                <span class="n">mean_cost</span> <span class="o">+=</span> <span class="p">[</span><span class="n">train_rbm</span><span class="p">(</span><span class="n">batch_index</span><span class="p">)]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training epoch </span><span class="si">%d</span><span class="s1">, cost is &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mean_cost</span><span class="p">))</span>
            <span class="n">pseudo_likelihoods</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mean_cost</span><span class="p">)</span>

        <span class="n">end_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
        <span class="n">pretraining_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">-</span> <span class="n">plotting_time</span>

        <span class="k">if</span> <span class="n">output_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">visbias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hidbias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">output_final</span><span class="p">,</span> <span class="n">visbias</span><span class="o">=</span><span class="n">visbias</span><span class="p">,</span>
                        <span class="n">hidbias</span><span class="o">=</span><span class="n">hidbias</span><span class="p">,</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
                        <span class="n">pseudo_likelihoods</span><span class="o">=</span><span class="n">pseudo_likelihoods</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training took </span><span class="si">%f</span><span class="s1"> minutes&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">pretraining_time</span> <span class="o">/</span> <span class="mf">60.</span><span class="p">))</span></div>

<div class="viewcode-block" id="RBM.sample"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s1">&#39;rbm_samples.npy&#39;</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
               <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">include_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param test_x: the test set, used for the starting points</span>

<span class="sd">        :param n_chains: n. of parallel Gibbs chains to be used for sampling</span>

<span class="sd">        :param n_samples: number of samples to plot for each chain</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy_rng</span>

        <span class="n">test_set_x</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
        <span class="n">number_of_test_samples</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># pick random test examples, to initialize the persistent chain</span>
        <span class="n">test_idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">number_of_test_samples</span> <span class="o">-</span> <span class="n">n_chains</span><span class="p">)</span>
        <span class="n">persistent_vis_chain</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
                <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span>
                    <span class="n">borrow</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="n">test_idx</span><span class="p">:</span><span class="n">test_idx</span> <span class="o">+</span> <span class="n">n_chains</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># define one step of Gibbs sampling (mf = mean-field) define a</span>
        <span class="c1"># function that does `plot_every` steps before returning the</span>
        <span class="c1"># sample for plotting</span>
        <span class="p">(</span>
            <span class="p">[</span>
                <span class="n">presig_hids</span><span class="p">,</span>
                <span class="n">hid_mfs</span><span class="p">,</span>
                <span class="n">hid_samples</span><span class="p">,</span>
                <span class="n">presig_vis</span><span class="p">,</span>
                <span class="n">vis_mfs</span><span class="p">,</span>
                <span class="n">vis_samples</span>
            <span class="p">],</span>
            <span class="n">updates</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gibbs_vhv</span><span class="p">,</span>
            <span class="n">outputs_info</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">persistent_vis_chain</span><span class="p">],</span>
            <span class="n">n_steps</span><span class="o">=</span><span class="n">plot_every</span>  <span class="c1"># scans many times; sample_fn picks the last</span>
        <span class="p">)</span>

        <span class="c1"># add to updates the shared variable that takes care of our persistent</span>
        <span class="c1"># chain :.</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">persistent_vis_chain</span><span class="p">:</span> <span class="n">vis_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]})</span>
        <span class="c1"># construct the function that implements our persistent chain.</span>
        <span class="c1"># we generate the &quot;mean field&quot; activations for plotting and the actual</span>
        <span class="c1"># samples for reinitializing the state of our persistent chain</span>
        <span class="c1"># takes nothing as an input and returns the last part of the sample</span>
        <span class="k">if</span> <span class="n">include_hidden</span><span class="p">:</span>
            <span class="n">sample_fn</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
                <span class="p">[],</span>
                <span class="p">[</span>
                    <span class="n">vis_mfs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">vis_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">hid_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">],</span>
                <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sample_fn&#39;</span>
            <span class="p">)</span>

            <span class="c1"># create a space to store the image for plotting ( we need to leave</span>
            <span class="c1"># room for the tile_spacing as well)</span>
            <span class="n">sample_data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_chains</span><span class="p">,</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_fn</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
                <span class="p">[],</span>
                <span class="p">[</span>
                    <span class="n">vis_mfs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">vis_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">],</span>
                <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sample_fn&#39;</span>
            <span class="p">)</span>

            <span class="c1"># create a space to store the image for plotting ( we need to leave</span>
            <span class="c1"># room for the tile_spacing as well)</span>
            <span class="n">sample_data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_chains</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="c1"># generate `plot_every` intermediate samples that we discard,</span>
            <span class="c1"># because successive samples in the chain are too correlated</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; ... getting sample </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">include_hidden</span><span class="p">:</span>
                <span class="n">vis_mf</span><span class="p">,</span> <span class="n">sample_v</span><span class="p">,</span> <span class="n">sample_h</span> <span class="o">=</span> <span class="n">sample_fn</span><span class="p">()</span>
                <span class="n">sample_data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">sample_v</span><span class="p">,</span> <span class="n">sample_h</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">vis_mf</span><span class="p">,</span> <span class="n">sample</span> <span class="o">=</span> <span class="n">sample_fn</span><span class="p">()</span>
                <span class="n">sample_data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample</span>

        <span class="c1"># construct image</span>
        <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">sample_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sample_data</span></div>

<div class="viewcode-block" id="RBM.vis_energy"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.vis_energy">[docs]</a>    <span class="k">def</span> <span class="nf">vis_energy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="n">J</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span>
        <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vbias</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hbias</span>
        <span class="n">exponents</span> <span class="o">=</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">J</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">exponents</span><span class="p">)))</span></div>

<div class="viewcode-block" id="RBM.savenp"><a class="viewcode-back" href="../../../../../neurotools.models.rbm.rbm_utils.rbm.html#neurotools.models.rbm.rbm_utils.rbm.RBM.savenp">[docs]</a>    <span class="k">def</span> <span class="nf">savenp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                    <span class="n">hidbias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span>
                    <span class="n">visbias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vbias</span><span class="o">.</span><span class="n">get_value</span><span class="p">())</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, M Rule

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>